# =========================
# Spark Defaults (updated)
# =========================

# --- Iceberg extensions (จำเป็นสำหรับ Iceberg SQL) ---
spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions

# --- Master / UI (ตามที่ใช้งานอยู่) ---
spark.master=spark://spark:7077
spark.ui.port=4040
# Worker UI (สำหรับ standalone worker)
spark.worker.ui.port=8081

# --- S3A (MinIO) ---
spark.hadoop.fs.s3a.endpoint=http://minio:9000
spark.hadoop.fs.s3a.access.key=test
spark.hadoop.fs.s3a.secret.key=test12334567
spark.hadoop.fs.s3a.path.style.access=true
spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem
spark.hadoop.fs.s3a.connection.ssl.enabled=false
# ตัวเลือก: ใช้คีย์แบบ static ชัดเจน
# spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider

# =========================================================
#  A) คงคาตาล็อกเดิม (ผ่าน Hive Metastore @ hive-metastore:9083)
#     ใช้ชื่อเดิม: iceberg และ lh เพื่อไม่ให้กระทบสคริปต์เก่า
# =========================================================
# alias: iceberg
spark.sql.catalog.iceberg=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.iceberg.type=hive
spark.sql.catalog.iceberg.uri=thrift://hive-metastore:9083
spark.sql.catalog.iceberg.warehouse=s3a://lakehouse-data/warehouse/

# alias: lh
spark.sql.catalog.lh=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.lh.type=hive
spark.sql.catalog.lh.uri=thrift://hive-metastore:9083
spark.sql.catalog.lh.warehouse=s3a://lakehouse-data/warehouse/

# =========================================================
#  B) เพิ่มคาตาล็อกแบบ HadoopCatalog (ไม่พึ่ง HMS) — ทดสอบ/ใช้งานแทน HMS ได้
#     ชื่อ: iceh   (ชี้ไปที่ warehouse เดียวกันบน MinIO)
#     ตัวนี้ช่วยเลี่ยงปัญหาเวอร์ชัน HMS กับ Spark/Iceberg ได้
# =========================================================
spark.sql.catalog.iceh=org.apache.iceberg.spark.SparkCatalog
spark.sql.catalog.iceh.type=hadoop
spark.sql.catalog.iceh.warehouse=s3a://lakehouse-data/warehouse/

# --- (Optional) ปรับทรัพยากรตามเครื่อง ---
# spark.driver.memory=2g
# spark.executor.memory=2g
# spark.sql.shuffle.partitions=8
